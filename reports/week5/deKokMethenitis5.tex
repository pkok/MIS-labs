\documentclass[a4paper,10pt,twoside]{article}

\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage[ocgcolorlinks]{hyperref}
\usepackage{todonotes}
\usepackage{caption}
\usepackage{subcaption}

\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[RE,LO]{de Kok \& Methenitis}
\fancyhead[RO,LE]{Week 5 \& 6}
\fancyfoot[RE]{\thepage$\quad \square$}
\fancyfoot[LO]{$\square \quad$\thepage}

\title{Report for Week 5 \& 6\\\normalsize Create your own ``Google Similar Images'' system}

\author{Patrick de Kok (5640318) \and Georgios Methenitis (10407537)}

\begin{document}
\maketitle
\thispagestyle{empty}

\section{Implement $k$-NN classifier}

\begin{figure}
  \begin{subfigure}{.47\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{knn_goal15}
    \caption{Query image \texttt{goal/15.jpg}.}
  \end{subfigure}
  \hspace*{\fill}
  \begin{subfigure}{.47\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{knn_bicycle37}
    \caption{Query image \texttt{bicycle/37.jpg}.}
  \end{subfigure}

  \begin{subfigure}{.47\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{knn_beach31}
    \caption{Query image \texttt{beach/31.jpg}.}
  \end{subfigure}
  \hspace*{\fill}
  \begin{subfigure}{.47\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{knn_mountain37}
    \caption{Query image \texttt{mountain/37.jpg}.}
  \end{subfigure}
  \caption{Query images and their $k=9$ nearest neighbours with labels.}
  \label{f:knns}
\end{figure}

\section{Different $k$-NN strategy}
A possible alternative to this simple voting mechanism will be by weighing each vote based on its rank.  Each vote with rank $r$ will be assigned a weight \[ w_r = \frac{1}{1+r}. \]  Images with a higher rank should be considered as more important than lower ranked images.  The addition of 1 to the rank in the denominator lets the first vote have less relative significance to the second and third votes, such that $w_1 < w_2 + w_3$.

Another weighing mechanism would be based on the global label frequency.  Votes for each label would be weighed by: \[ w_\ell = \frac{1}{\#\ell} \] where $\#\ell$ represents the frequency of label $\ell$ occurring among the training images.  One might want to normalize these weights before they are used, such that \[\bar w_\ell = \frac{w_\ell}{\sum_{\ell \in L} w_\ell}. \] Here, $L$ is the set of all labels.  This ensures that \[\sum_{\ell \in L} w_\ell = 1.\] By weighing votes of very frequent labels less and very infrequent labels more, we tell the system that when a rare (or common) label is suspected to occur in the image this is very (or less) significant than expected.

With training sets of limited size, one is often interested in applying a smoothing method over these weights, such as Good-Turing smoothing.  This will even out the pure chance occurance of very frequent labels and very infrequent labels.  Because this dataset has equal frequencies for all labels, this method is not applicable, and only the first method is implemented.

Accuracy for both implementations can be found in Table~\ref{t:knnacc}.  One can see that the original implementation performs slightly better.  When inspecting the nearest neighbours of the four queries visually, it becomes clear that the histogram intersection distance is not appropriate for this.  The best label is most often not the first returned by $k$-NN.

\begin{table}
  \centering
  \begin{tabular}{l|rr}
                       & 4 queries & 100 queries \\
    \hline
    Original $k$-NN    & $0.5$     & $0.31$ \\
    Alternative $k$-NN & $0.25$    & $0.26$ \\
  \end{tabular}
  \caption{Accuracy of the $k$-NN classifier with both voting schemes.}
  \label{t:knnacc}
\end{table}
\section{Implement mean class accuracy}

\section{Cross validation of $k$}

\section{Evaluation of 4 possible classifiers}

\section{Use scikit}

\section{Ring data -- linear SVM}

\section{Non-linear transformations}

\section{Transform to polar coordinates}

\section{Non-linear SVM}

\section{Image classification -- linear SVM}

\section{Image classification -- discussing the results}

\section{Image classification -- comparison $k$-NN vs SVM}


%\bibliographystyle{plain}
%\bibliography{thereference}
\end{document}
